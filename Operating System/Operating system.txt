1.) Types of OS
    > Batch
        > jobs
        > Punch Card, Magnetic Tapes
        > not real time Operations

    > Multiprogramming
        > Non-Preemptive
        > Multiple process in RAM though
        > High response time

    > Multitasking (Multiprogarmming with Time Sharing)
        > Preemptive
        > Low response Time

    > Real Time OS
        > O/P in real time.
            > Hard : no delay tolerated(Missile system)
            > Soft : small delay tolerated(Youtube livestream)

    > Distributed
        > various devices connected in synch
        > facebook data
        > one system fails , other can do the job
        > Cloud Computing

    > Clustered
        > various devies connected in synch in LAN
        > extension of distributed more powerful
        > super computers

    > Embedded
        > fixed functionality
        > Washingmachine, Oven etc.
        > HardWired

2.) Process States :

        > New ==> Ready ==> Running(==> Wait/Break==> Ready) ==> Terminated

        > New
            > Created process
            > from Secondary Memory

            > from New to Ready is Done by LTS(Long Term Scheduler)
                > LTS achieves optimal Multiprogramming(getting as many process in ready state as possible)

        > Ready
            > in RAM
            > Queue(FIFO)

        > Running
            > Short Term Scheduler(Dispatches processes) brings process in running state(Processor starts reading the process from RAM )
            > How many process come depends on no. of processors


        > Terminated
            > Deallocation of resources

        > Wait/Block :
            > Waiting is in Ram only
            > in multitasking, process can go to wait.
            > reasons of Wait ??
                > higher priority process comes
                > process needs non processor resource (IO request)
                > Time quantum(max time that can be allocated) expires
            > after wait process goes to ready state

        > EXTRA STATES :
            > Suspend Wait
                > Suppose Wait/Block state queue is full, the process is sent to Suspend Wait that is back to secondary memory
                  until queue is free of space.
                > Done by Medium Term Scheduler(MTS)
                    > this scheduler sends process to secondary memory in case ram is full.
                > process can come back once there is space.

            > Suspend Ready
                > Suppose a high priority process comes inn and the Ready Queue is full, in this case the less priority process are sent to
                  Suspend Ready state which is secondary memory.
                > Done by MTS only
                > process can come back to Ready Queue once there is space.

            > Backing Store :
                > Just in case, A process in Suspend Wait is unable to get back to Wait state queue(this might be due to ready queue being full
                  for a long time and no space is free in wait queue for a long time), the process can jump to Suspend Ready state.
                > this is called Backing Store

>> OS vs Kernel :
    > Operating System:
        It is a system program that provides interface between user and computer. When computer boots up Operating System is the first program that loads.
        OPERATING SYSTEM IS THE INTERFACE BETWEEN USER AND COMPUTER.

    > Kernel:
        A kernel is a subset of OS which plays role in executing system calls and here is where the various OS functionality is performed.
        KERNEL IS AN INTERFACE BETWEEN PROGRAM AND HARDWARE.

>> Non Preemptive
    > no waiting time and process in running state stays till executed(multiprogramming without time sharing)

>> Preemptive
    > waiting time and process in running state can be put to waiting state(Multiprogramming with time sharing or Multitasking)


3.) System Calls :
    > Usermode : this is for basic programming without any means to access the functionality of os
        > inturrupts are generated by programs in user mode.
    > KernalMode : this mode is for making use of OS functionality
        > eg. when we want to access a device such as printer, so we will kave to go to kernel mode.

    > System calls are for kernal mode and for managing the OS functionalities.
    > NOTE : eg. printf() is a function which accesses the monitor resource, so printf is a function that makes use of system call Write().
    > diferent OS have different number of system calls.
    > Types of System calls :

        > File Related : for file related functionalities. eg. Open(),Write(),Close(),CreateFile()etc.
          Suppose a program wants to access a file, a normal program does not have access to System calls and hence the program will have to evoke system calls
          with the help of kernal(which is part of os and os also stays in ram like normal processes).

        > Device Related : these are for accessing resources like printer, monitor, speaker etc.
                            eg. Read,Write,Reposition

        > Information Related : when we want to access the information regarding various process, we use the information related system calls.
                            eg. getPid(to get process id),getPPid(to get parent id), getSystemTime.etc.
          this data we access is meta data since its data about data

        > Process control : for various control of process that are to be executed or are being executed.
                            eg. Load,Execute,aboart,Fork,Wait,Signal,Allocate etc.
                            Fork : fork system creates a child process of a parent process and hence can help in multiprocessing environment(kinda like multithreading)
          in this note how we can control how a process is treated and how we can control the state of a process.

        > Communication : for intercommuncation between various process this is used.
                            eg. pipe(), create/delete connections, shmget()[to get shared memory value].
          this is so two dependant process can communicate with one another.

        > Others : the above are the basic ones but there exist various other sustems calls cateogory like : security and protection related etc.

>> Fork() System calls :
    > used to create a child process.
    > child process is a clone of parent and has a seperate id.
    > how fork works is that if the parent is buisy, child can perform another task.
    > this is different form multithreading where instead of dividing a process , a clone is made.
    > Fork() returns 0 , +1 or -1
        > 0 returns child process id
        > +1 or any positive number returns a parent process id
        > -1 or any negative number is when child could not be created due to reasons like kernal buisy(generally not considered since a child is always created)

    > eg.
      void main()
      {
        fork();
        printf("Preetam");
      }

      in the above case since fork() is inside of a parent , a copy of main function will be made which will be child, and both parent and child will run
      concurrently(simultaneously) and print Preetam Preetam.

      void main()
      {
        fork();
        fork();
        printf("Preetam");
      }

      in the above case the first fork will create a child of main function, and since the main function still has a fork inside, the initial parent and child will
      have their own child aswell. hence there will be 3 child class of one parent and all the parent and child will run simultaneously printing Preetam Preetam Preetam
      Preetam.

      in case of 3 fork() : there will be 7 copies of main process and hence one parent and 7 child. and Preetam willl print 8 times.
      hence 2^n where n is the number of times fork is used.

      see : forkSystemCallQuestion.png
      void main()
      {
        if(fork()&&fork())
        {
          fork();
        }
        printf("Preetam");
      }
      so how many times will Preetam print??
      fork() when called will make a child and also along with that return the value +ve or 0 based on if its inside of parent or child
      so 1st fork will make a child and child and parent will run simultaneously. so in child, if will be false (and since the 1st fork() is 0,
      it will not go to the next fork since thats the property of AND operator.) and in the parent the case will be true the 2nd fork will also execute making
      another child and parent. This is a bit complex so its better to wathch the video reference to understand the question better

      remember upon encountering fork a child is generated but the fork command becomes 0 or +ve. i.e. fork command that made the child will not be in th child.

4.) Usermode VS Kernal Mode :
    > Usermode if for normal usage and non OS related usage
    > Kernal mode is for Operating system related functionalities.
    > for reading a file or writing in a file we canot access the memory directly since it is a resource that is managed by OS so we instead interact with memory
      using kernal or we can say kernal mode
    > Mode bit of UserMode is 1 and that of KernalMode is 0.
    > a program working on usermode  gets a system call and sqitcher to kernal mode where the system call is executed and then return to usermode.
    > in short an application running on usermode need kernal mode to interact with the h/w and hence will have to go through OS.

    > eg. suppose we want to perform 2+2 and display it. now the addition is done in usermode but now interaction with h/w(monitor) is needed to print the result.
      hence kernal mode is needed.

    > THE OS WORKS ON KERNAL MODE AND NON SYSTEM PROGRAM WORK ON USER MODE. SWITCHIG TAKES PLACE IN BETWEEN.

    >> The system is in user mode when the operating system is running a user application such as handling a text editor. The transition from user mode to kernel mode
    occurs when the application requests the help of operating system or an interrupt or a system call occurs.

5.) Process vs Thread in OS :
    > Process :
        > a process as whole
        > system call involved since process operations are performed using System calls in kernal.
        > OS treats different process differently.(seperate process will have seperate pid)
        > when copy of process is made with fork, seperate child and parent process will have seperate code,files and data.
        > Context switching is slow.
        > Blocking a process will not affect other.
        > CANNOT UTILISE MULTIPROCESSING
        > Independent.

    > Thread :
        > a small process which is part of a bigger process.
        > System calls not involved since threading is done by Application.
        > threads are treated as a single process by OS.(threads of a process will have same pid)
        > threads share same data, code and file.(stack and registers are seperate though.)
        > Context switching is fast.
        > Blocking a thread blocks whole process.
        > CAN UTILISE MULTITHREADING PROPERLY.
        > Interdependant.

5.) Userlevel threads vs kernal level threads :
    > Userlevel threads
        > user level threads are managed by userlevel threads and are managed on the application level.
        > Userlevel threads are comparatively fast.
        > Context Switching from thread to thread is faster
            > since it is done within the application, its faster comparatively
        > If one thread performs blocking(eg. going to resource), entire process gets blocked.
        > same as kernal level threads, these threads share code and data but not the register and other things.
        > not utilise multiprocessing.
        > the main use of user level threads is to reuse the code and not utilise multiprocessing.

    > Kernal level threads
        > Kernal level threads are managed by OS(Hence System calls are used to manage these)
        > Kernal level threads are slower than user level threads.
        > Kernal level threads are made similar to process(using system calls.)
        > context switching is slower.
            > the context switching is done by the processor and hence this is slower since lots of stuff is to be done.
        > if one thread is bloacked, no affect on others.
        > Kernal level threads share code and data but not the register and other things
        > True utilisation of Multiprocessing
        > true utilisation of kernal level threads is achieve multiprocessing.

6.) Scheduling Algorithms :
    > Scheduling algox is for evaluating the sequence of putting process on running state from ready queue based on requirement and optimalisation.
    > note that ready queue is in the ram.
    > There are two types of Scheduling algox brodly :
        > Non Pre Emptive : Non PreEmptive means, a running process cannot me made to leave the state until it finishes.
        > Pre Empive : Pre Emptive menas that a running process can me made to leave running state into ready queue.
            > reasons of removing from running state :
                > Time Quantum expires
                > Higher priority process comes
                > the process is in need of other resources.

7.) Pre Emptive vs Non Pre Emptive :
    > Pre Emptive :
        > SRTF(Shortest Remaining time first)
        > LRTF(Longest Remaining time first)
        > Round Robin
        > Priority Based (can be a part of non preemptive also)

    > Non Pre Emptive :
        > FCFS(First Come First Serve)
        > SJF(Shortest Job First)
        > LJF(Longest Job First)
        > HRRN(Highest Response Ratio Next)
        > Multilevel Queue
        > Multilevel Feedback queue
        > Priority Based(Can be a part of preemptive also)

8.) Terminology :
    > Arrival time : Time process enters ready Queue(Time)
    > Burst Time : Total Time spent by a process on Processor(Time Period)
    > Completion Time : Time at which process exits or terminates.(Time)
    > Turn Around Time : Total Time spent by a process from getting loaded in ram and getting executed(Time Period)
        this is like sum of time on processor, time spent waiting, time spent on using resources.
        TAT = Complition time - Arrival Time
    > Waiting time : time that a process had to wait in readyqueue or in waiting before getting the processor(Time Period)
        we can also say, that waiting time is time period a process lived outside of the processor.
        Hence waiting time = TAT - BURST TIME.
    > Response Time : average time period in which a process gets processor after arriving for the fist time.
        we call a algorithm having a good response time, if it executes a process and then seitches to another quickly
        it is far better for processor to switch from process to process quickly rather than spending significant time
        on a single process.
        RT = time instance a process gets cpu - arrival time

>> IN non preemptive alox, the WT is equal to RT.

    > CPU bound and IO bound : a process has time needed in the preprocessor and a time needed on the resources.
        if a process has more need of IO than processor, than its complition time is dependant on the IO speed, and hence
        its IO bound. Similarly if a process has more time needed in the processor than the io, then the speed of its
        complition is dependant on the speed of the processor and hence its CPU bound.

        IN os we dont consider the io.

    > Average Waiting Time : Average of all the waiting time of processes.
    > Average Response Time : Average of all the Response time of the processes.
    > Average TAT :).

9.) FCFS :
    > First Come First Serve :
        > Criteria : Arrival Time
        > Mode : Non-Preemptive
    > Here Arrival time determines the sequence in which the process will be executed.

NOTE : TWO PROCESS CANNOT EVER ARRIVE AT THE SAME TIME !!!!! but if it comes in the question, we can choose any first.

    > Chart used to represent the time , is called a Gantt chart.

10.) SJF :
    > Shortest Job First :
        > Criteria : Burst Time
        > Mode : Non Preemptive

    > the job with the shortest time required on the processor gets executed first.
      note that the process that enters the cpu will only leave once it has executed.
      incase two process have the same burst time, the one with the lower arrival time gets executes first.

11.) SRTF
    > Shortest Remaining Time First(SJF with preemptiveness)
        > Criteria : Remaining Burst Time(remaining time left to execute on CPU)
        > Mode : Preemptive

    > in this case we cannot make a process completely execute , since anoter process having burst time smaller than the remaining
      burst time of the process may be there and that will be executed.
      this is checked every single instance and the moment a process with smaller time comes,boom, context switch.

12.) Round Robin :
    > Criteria : Time Quantum
    > Mode : Preemptive

    > Time Quantum : a time period for which a process can run max at a time before the context is switched.
    > in this its always prefered to maintain 2 queues or gant charts.
        > Ready Queue : Ready queue is maintained using the arrival time since arrival time is when a process enters ram or readyqueue.
        > Running Queue : Running queue will maintain the running processes.

    > Context Switching : Context switching is , saving or applying the current info and calling in a new process.

>>NOTE :     PCB is the process control block which keeps all the info of the process. when a process returns to processor, it is resumed and not restarted
        hence its context is saved when context switching. or we can say process info is saved in PCB when a process removed from context.
        PCB actually keeps track of the instruction a process was last time it was in the processor.
        Each Instruction has its own address.
        Now the PCB loads the to be executed Instruction address in the Program Counter .
        Program Counter is a location where the address of currently executing Instruction is stored.

    > ready queue is a queue and hence the process that gets context switched , joins the queue from the endpart.
    > NOTE : incase a process comes into readyqueue and a process context switches at the same time, which one will join the readyqueue first ?
        the fresh entering process will join the queue first then the context switched one.

13.) Priority Scheduling with Preemption :
    > Criteria : Priority
    > Mode : Preemptive

    > note that the first step is identifying which is higher priority , the larger number or the smaller number. eg. if 1 is higher in priority or 10

    in this 3 things are given, Priority, Processes arrival time, Burst time.

    > the process with highest priority gets executed and if a process with higher priority comes, context is switched.

>> in gate upsc, question can arrive with questions which have initial burst time, then IO time and then final burst time, Such question has been dealt with
    in one example in the image.

14.) Multilevel Queue Scheduling :

    > When there are various types of process, there are various ready queues for different types of process in real life
      this is the concept of Multilecel Queue Scheduling.

    > Type of Process Queue :

        Highest priority : SYSTEM PROCESS ---------RR-------->
                                                             |
        Medium  Priority : INTERACTIVE PROCESS ----SJF----> CPU
                                                             |
        Lowest  Priority : BATCH PROCESS ----------FCFS------>

        System process are the most important system process whereas Interactive process are the medium priority process like video streaming etc.
        Batch Process are the lowest priority process which are executed in groups.

        Hence there are seperate queues for seperate types of processes.
        Also , seperate process queues have seperate scheduling algorithms.


      > DISADVANTAGE :
          > this has Starvation : Starvation is when a process has to wait for very big amounts of time and in some cases infinite time for getting the Processor.

          > if high priority process keep coming in the Queue say System process keep coming in the queue, the medium priority and low priority queue process will
            Starve.

15.) Multilevel Feedback Queue :

    >  In multilevel feedback queue, there are various ready queues ascending in order in priority.
        Each queue has a set time quantum.
        now if a low priority process enters it will enter the least priority queue and after executing for the time quantum set for that queue,
        will be sent to one priority above queue. in this way a process will keep moving to higher queue which have a higher time quantum also and sort of
        keep getting promoted until they are done.

        high level process like system process will not go through this since they will directly join the highest priority queue.
        and medium priority process will  join the medium priority queue and then move up.

    > for better understanding , see the respective image.

    > Usin this queue, there is no starvation in low level process and also the high priority process dont get disrupted.

>> HIGHEST response ration first :
    > it is done the same way as higherst priority first, but instead response ration is checked and is non preemptive.

    Criteria – Response Ratio
    Mode – Non-Preemptive

    Response Ratio = (W + S)/S = TAT /S
    Here, W = waiting time
      S = Burst time

16.) Process Synchronisation :

    > Series and Parallell : process can run together at the same time, or parallell to each other.

    > Process are of two types in Parallell concept :
        > Cooperative Process :
            > Cooperative processes are those process in which execution of one depends on each other.
            > this is because they share something.
                > eg share variable
                > Memory
                > Code
                > Resources eg. scanner , cpu, Scanner etc.

            > if there is anything common in two process, then they are cooparative

        > Independent Process :
            > Process that are not dependant on each other.
            > WE dont need to study these.

    > Process Synchronisation is very important in Cooperative Process
    > Cooparative process when not executed in synchronised why, lead to lots of issues.
    > in the image problem,two process running at the same time share int shared
        > even though the desired result is 5, it either gives 6 or 4.
          This is because the process dont run in synchronisation with each other resulting in this.

        > Such a condition is called race condition , in which two dependant process execute to get their part of the job done first while whereas they
          should work in harmony to achieve the result.

17.) Producer Consumer Problem(need of synchronisation) :
    > there are two cooparative process one form consumer side and one from the producer side .
    > they are called consumer and producer since the producer code is making something and the consumer is using it.
    > since they are cooparative, they share some resources.
        > in this problem there is producer code which runs and makes an item and places it in a buffer.
        > the buffered is shared by both the processes.
        > they also share a count variable.
        > now the consumer does its work and then use the produced item.
    > the catch is , that the buffer is shared memory and the producer adds to buffer while consumer takes form the buffer.
      and at a time only one can access the buffer.
    > the problem is that the consumer should not be able to consume from empty buffer and producer should not add to full buffer.
    > solution can be, to make the producer sleep when buffer is full and to be awoken when the consumer consumes form the buffer.
      similarly to make the consumer sleep when buffer is empty and to bring it back once the producer produces something.
    > while(count == 0); : note this code will stay stuck unless count is incremented, hence this can be used.

    > caution: improper logic can lead to a deadlock where both producer and consumer process are asleep and waiting for each other.

18.) Printer Spooler Problem :
    > not just printer, this is for all slow peripheral device which needs to be accessed by multiple users.
    > not multiple users are accessing the printer at the same time and hence we need Spooler which is a program of the printer.
        > Spooler has a directory which keeps all the document printing requests .
        > the spooler will sequentially send all the requests to printer.
        > this is just like a buffer which keeps the track of multiple peripheral operations requested from printer.
    > SPOOL : Simultaneous Peripheral Operations Online.
    > What happens when two process arrive at the same time in spooler ??
        > this happens when the two process are parallell process.
        > the spooler determines the sequence of printing based on arrival time, and since both the process arrive at the same time
          there is loss of data since one of the two process data is not stored by the spooler.
    > this explains the need of process synchronisation.

19.) Critical Section Problem :
    > this problem occures when multiple process try to access same piece of shared resource at the same time.
    > for obvious reasons this happens in cooparative process.
    > critical section refers to the shared resource whereas the remaining is non critical.
    > How do we deal with critical section ??
        > the non critical code is executed by a process normally but the Critical section is not executed directly. I.E. there is entry section before critical section.
        > this entry section is just code that has logic for process synchronisation
        > so long story short, before the critical section code there is process synchronisation code which places condition on the critical code so that the code is
          executed in a synchronised manner.
        > this code has semaphores, monitors etc for this kind of stuff for process synchronisation.
        > hence this entry code makes such that, if a process is in the critical section, another cannot enter the critical section.
        > the process has left the ciritical section, it goes to exit section and exit section means that anoter process can now use the critical section.
    > if entry section and exit section are not used, there can be race condition.
        > race section is when cooparative process run selfishly without interacting to get their job done and hence causing data loss and other types of issues.

20.) 4 Conditions/rules for process synchronisation :

    > there are primary(mandatory) and secondary(non mandatory but recomended) rules :

    > Mutual Exclusion(primary) : if critical section is in use by one process, other process cant use it.

    > Progress(primary) : this issue is when a process trying to access critical section is unable to access it even though no one is using it because of another process.
                          this occures when the entry section of a process is dependant on another process. hence to let there be progress, the entry section should not be
                          dependant on other process.

    > Bounded Weight(secondary) : there should be some bound or quantum that must be assigned to a process so that it does not access the critical section infinite time without
                          giving chance to other process. Hence there should be a limit a process can continuosly access the CS so as to remove partiality.

    > No assumption related to hardware or speed(secondary) : the critical section selection should be hardware indipendant and should be applicable in all hardware types.
                          and should be portable.

21.) Semaphores :
    > Tool used to prevent race condition.
    > Semaphore is an integer variable which is used in mutual exclusion manner by various concurrent cooperatice process in order to achieve synchronisation.
    > Semaphore variables is global and hence shared by all processess.
    > Types of semaphores :
        > Counting Semaphore : form -infinity to infinity

        > Binary Semaphore : 1 and 0

    > the Entry section and exit section uses the following methods :
        > P()/Down/Wait : used in entry code,this method is run before a process can access critical section.
        > v()/Up/Signal/Post/Release : used in the exit section, this code is run after a process has used the critical section.

    > so what happens is :
        in a mutual exclusive Critical section, the value of semaphore is 1 when the critical section is not in use.
        once a process wants to use critical section , it goes through the P() method and the value of semaphore is made 0.
        if anoter process tries to access the critical section while it is in use already, an if condition will put the process to sleep since the value of semaphore
            is 0;
        when the first process has completed the execution, it will fo through v() method that will make the semaphore value 1 meaning that the Critical Section can be accessed
            by anoter process again.
        also note that multiple process will come and go to wait and also make the semaphore value more negative and by the negative number we can determine the number of
            process that are in wait.
        we can slove the producer consumer problem and achieve process synchronisation using counting semaphores.

    > Binary Semaphore :
        binary semaphore has 2 states , 0 and 1
        counting semaphore is not used and instead binary semaphore is prefered.
        binary semaphore value cannot go lower than 0 and hence its very simple.
        the rest of the process is same.

22.) Deadlock (not the wwe one)

    Deadlock is situation in which two or more event are depending on an even to happen so that they can execute, but that event does not happen, this is called a deadlock
    and the process are said to be in a state of deadlock.
    eg.
      suppose p1 and p2 are two process . also there are 2 semaphores s1 and s2. now p1 is using semaphore s1 but also needs s2 and p2 has semaphore s2 but also needs s1.
      in this situation both the process will keep waiting for the semaphore but no semaphore will arrive hence causing deadlock.
      also since s1 and s2 are in use, the process will be sent to block state and will be in that state unless the semaphore is 1 but that will not happen and Hence
      eternal tsukinomi is achieved.

    there are 4 necessary condition for deadlock :
        > Mutual Exclusion : the resource causing deadlock must be mutual exclusive, i.e. only one process can access it at a time.

        > No Preemption : the resource must not be preemptive , i.e. once a process has a resource, it will hold onn to resource unless its executed.

        > Hold and Wait : the process must hold the resource and is capable of waiting for another process.

        > Cricular Wait : there should be a circular formation in resource process diagram.

        all of the above must be satisfied for deadlock to happen.

23.) Resource Allocation Graph (RAG) :
    > for representing state of a system it is used.
        > like how the resources have been allocated to the process.
    > this is most suitable to check if there is deadlock.
    > a graph has 2 things :
        > Veretx(node) :
            > vertex reprenest the process and Resource
            > process is represented in a circle
            > resource in a box with a dot.
                > number of dots in a resource vertex represents the number of that resources available
                > eg. motitor will have 1 dot and registers will have multiple

        > Edges(branches) :
            > Edges are represented using arrows and mean assigning and request.
            > if arrow goes from resource vertex to process vertex, its Assigning(obvious) meaning the resource is there with process.
            > if arrow is going from a process to a resource, it means the process is asking for the resource hence Request.

    > We can check looking at a graph if there are deadlock conditions satisfied.

    > Resource having multiple instances(multiple dots ) can be allocated to multiple process.
    > later making a table of allocated and request using the RAG, we will use bankers algorithm to find deadlock.

24.) Deadlock Handling :
    > Deadlock ignorance(Ostrich method) :
        > used by windows and linux and as the name suggests, ignore the deadlock.
        > most used deadlock prevention method LMAO.
        > just like how in system if program crashes, system crashes and we restart or poweroff.
        > this is because deadlock is very uncommon, and writing a fullfledge code to handle deadlock. since it takes resource and speed.
        > so in this we dont bother to add more code to remove the deadlock NEAT.
              > SPEED MATTERS BIATCH

    > DeadLock Prevention (before process are running):
        > we can write code to deal with deadlock before it occures, i.e. while making the programms.
        > this can be done by preventing any 1 of 4 conditions needed to  get deadlock.
            > we can make resources sharable
            > we can make preemption
            > we can prevent circular wait
            > we can prevent hold and wait.

    > Deadlock Avoidance (after process start running) BANKERS ALGO :
        > in this everytime resource is given to a process, it is checked if its safe or not.
        > this is done using Bankers Algorithm.
        > disadvantage is that we have to provide all information regarding available resources etc beforehand and its very
            hard to implement.

    > Deadlock Detection and Recovery :
        > this detects deadlock at any point and tries going back to a better state which is recovery.
        > note that in Avoidance, deadlock was detected before allocating resources, this is done after the resource is allocated.
        > Various recovery methods can be :
            > Kill the processes or process
            > Resource Preemption , i.e. in deadlock the resource is preempted from one of the process to break circular wait.

25.) Banker ALgorithm / Deadlock Avoidance method :
    > this algo is also used for deadlock detection.
    > in the image :
        > Allocation : already resource allocated to process
        > MaxNeed : the maximum resources needed to execute.
        > Available : available resource available for a preocess.
        > Remaining need : Max Need - Allocation.
    > Finally we compare available and Remaining Need for all the process and make this deduction :
        > we check from top to bottom and if remaining need is satisfied, we free the process and add its remaining need in the available
        > so we keep going like that again and again adding the freed up process remaining need to available.
        > there will be 2 scinarios :
            > if all the process get executes, there will be no deadlock
            > if there are some process left and there is no process that can be executed due to small available, there will be deadlock.

26.) Memory Management :
    > it is the method of managing the primary memory and is responsibility of OS.
    > CPU is connected with RAM,Registers and Cache.
    > the ram can interact with secondary memory .

NOTE : Registers are faster than Cache memory. register are used for purpose like holding cpu output and input temporarily etc so there interaction with cpu is
    direct. Registers and Cache(SRAM) come attatched with the processor.

    > CPU cannot interact directly with secondary memory hence it gets the to be executed program from secondary memory into RAM.
    > Degree of multiprogramming is the number of process we can bring into ram at a time.
    > Degree of multiprogramming determines the CPU utilisation and is duty of OS since os brings programs from hdd to ram.

NOTE : only the process that need CPU need to be in ram since ram is the only memory cpu can interact with.

    > cup utilisation(out of 1) = 1 - (p1/100 * p2/100 * p3/100 ..... pn/100)
        > here p1,p2,p3,p4 are the percentage of time process spend time in IO operations.
        > we assume p1,p2,p3 etc are all process currently in ram.
        > observation is that the more number of process we are able to fit inside of ram , the better the cpu utilisation.
    > HENCE MEMORY MANAGEMENT HAS TO ENSURE THAT THE DEGREE OF MULTIPROGRAMMING IS AS HIGH AS CAN BE.

27.) Memory Management Techniques :
    > OS uses many techniques to get a better degree of multiprogramming.
    > Memory management techniques have to ensure that the ram is used in strategic way.
    > We can either copy the complete process into continuos memory block, or we can break a process into chunks and add them at random
        location in the ram.

    > Base on above, Memory management techniques are of 2 types :
        > CONTIGUOS :
            > Fixed/Static Partition.
            > Variable/Dynamic Parition

        > NON CONTIGUOS :
            > Paging
            > Multilevel Paging
            > Inverted Paging
            > Segmentation
            > Segmented Paging

28.) Contiguos Memory Allocation :
    > in this the programms are put into RAM without dividing into fixed number of parts.
    > Fixed/Static Partitioning :
        > in this , OS takes some part of ram, and the ram is divided into fixed number of slots. This is programmed fixed in the OS and the
            partition can be equal in size or unequal.
        > in a single partition only one process can wait.
        > it is hard coded and the number of partitions is fixed, but size is not fixed.
        > A partition will hold process of its own size or smaller size.
        > Also breaking a process is not allowed in this ,the whole process must fit in a partition.

>> INTERNAL FRAGMENTATION :
      > when a process arrives, in sequence OS checks any partition that can fit the process .
      > the first partition that can fit the process is selected and the process is placed.
      > now if the process was of 2 mb and the partition of 4 mb, there is wastage of 2 mb.
      > this is internal Fragmentation
      > Internal fragmentation phenomenon where unused memory is left in partitions in the RAM.
      > This leads to wastage of a very important resource because of contiguity, RAM.

>> EXTERNAL FRAGMENTATION :
      > due to internal fragmentation there are memory pockets left in various partitions.
      > now if the total of the internal fragmentation memory is 5mb and a process of 4 mb arrives, it cannot fit in ram.
      > this is due to the fact that the memory management techneque is CONTIGUOS in nature and wont let process be stored in non continuos chunks.
      > External Fragmentation is when a process cannot come into ram even if there is memory left due to internal fragmentation due to the
          contiguos memory management .

        > there are various disadvantage of static partitioning :
            > there is limit in process size since we cant bring process bigger than the max sized partition.
            > Internal Fragmentation
            > since number of partition is  fixed, the number of process that can come in ram at a time is fiexd.
                > Limitation on degree of Multiprogramming.

        > This was used in mainframe computers in 1960s.
        > in the coming techniques, we will improve upon this.

    > Variable/Dynamic Partitioning :
        > in this, the partitioning is done after the arrival of a process unlike static partitioning.
        > this leads to abolition of internal fragmentation.
        > the degreee of multiprogramming is very optimal in this.
        > but there is EXTERNAL FRAGMENTATION
            > suppose 2 process leave the ram creating 2 holes in the memory.
            > if the sum of the holes is 8 mb, it still cant accomodate an 8mb process because of continuos allocation .
            > even though the free space collectively is sufficient to fit the process if due to contiguos nature of allocation if we cant allocate a
              process memory, it is due to external fragmentation.
        > COMPACTION :
            > IN compaction the used up programms are moved in one side and free space is moved to another.
            > this will result in the free spaces accumulating and creating a bigger hole.
            > this is not a very good method though since :
                > if any of the process to be moved is running, it has to be stopped
                > the process have to be copied in new location which is time taking and not good.

        > Allocation and Deallocation is very complex. since in fixed partition, the starting address of every partition was fixed and hence
          allocation and deallocation was Easy. But in this , the partition are variable in size leading to compication.
        > BitMap and linkedLists are used to keep track of where process is and where hole is.

29.) Memory allocation techniques in Contiguos Memory Allocation :
    > Earlier we learned various ways memory can allocated , But how its done using which allocations??
    > Terminology and prologue :
        > discussing the algorithms used to allocate memory, it is assumed that some process are already in the cpu at random places.
        > holes are the spaces in between not taken.
        > algox show how to deal and how to allocate process to holes.

    > FIRST FIT :
        > allocate the first hole where the process can fit.
        > the next allocation check starts again from address zero.
        > very fast.(for obvious reasons)

    > NEXT FIT :
        > Same as above but start searching from the last allocated hole and not the top.
        > a pointer keeps track of where the last allocation of memory was allocated.
        > advantage is that we dont have to search from top. but what if suitable memory hole was near the beginning..... BURHHH

    > BEST FIT :
        > Search the entire list and search the hole that leads to minimum internal fragmentation.
        > this is more complex to implement.
        > Best fit can lead to very small holes left , holes that are useless.

    > WORST FIT :
        > we simply search the whole list and give it to the biggest holes.
        > in this , the holes created are big enough to accomodate another process and solve the issue of very small holes.
            > actually reducing external fragmentation

30.) Non Contiguos Memory Management :
    > in this , simply splitting the program into chunks is allowed.
    > accordin to size of hole, the process is divided accordingly.
    > SPLITTING A PROCESS IS VERY COSTLY.
    > ISSUE ??
        > Holes are created dynamically.
        > it takes a long time to split up a process looking at the available holes that are generated dynamically.
        > dynamically meaning it can be at any individual address !!!!
        > and also search for free slot will have to go to every single address.
        > also in the time of splitting what if structure of holes changes...
        > there are lots of issues and only one solution.
    > Solution :
        > What if we split the process and ram memory beforehand??
        > i.e. before sending a process into ram, split it in main memory and the main memory is also split into chunks.
        > welcome the concept of paging.

    > PAGING :
        > in non contiguos allocation , the process is split into smaller chunks in the secondary memory only and each chunk is called a Page.
        > the main memory is also split into partitions called Frames.
        > Size of Page = size of Frame. ==> always.
        > now when frames are present its very easy to keep track of holes since they are fixed size.
        > now searching for free holes is better since not every address needs to be traversed , but traversing is done using frames.
        > if slot are empty is better to add pages in continuos way.
        > EXTERNAL FRAGMENTATION IS ABOLISHED.
        > Since the addressing used is byte addressing, the unit for making frames or pages is byte.

        > CPU is unaware of pages and frames.(since this is done by OS)
        > The concept of paging is only for memory management, in actuality the CPU asks for perticular byte addressess.
        > how do CPU and OS view and make operations on addressess ??
            > The address that cpu requests from os , is not the absolute address.(it is logical address.)
            > since an address cpu asks with respect to the program can be in any frame.
            > This is why mapping is needed.
            > MMU(Memory Management Unit) : MMU converts the cpu generated address into absolute address.(physical address.)
                > it uses Page Table to keep track of which frame the address is present in.
                > each process has its own Page table.
                > how many entries a page table will have?? the number of pages present in that process. SIMPLE.

>> NOTE : CPU always works on Logical address aka. Virtual Address( Page Number + Page Offset) whereas OS works on main memory and uses
          Physical Address(Frame number + frame Offset)

>>> PHYSICAL ADDRESS VS LOGICAL ADDRESS AND PAGE TABLE :
    > Logical Address :
        > Logical address is the address that represents address of a program when its broken into pages.
        > Logical address comprises of Page Number(in binary) followed by Page Offset(in binary).
            > Page Number : when a process is divided into pages, page number represents the number at which page lies in series of pages.
            > Page Offset : offset is the count at which a particular byte is present.
        > ulitimately, Logical address represents a Byte address only just in a way that the first part of the address is the page number and
          the second part is the position in the page at which the byte is present.
        > Every seperate Process will have its own Pages and hence seperate logical addressing.
        > SUPPOSE logical address is 01001010(4 bytes page number and 3 bytes offset)
          we can say the following about this logical address :
            > 01001 = 9 and 010 = 2 hence the byte is in 9th page at 2nd spot.
            > since there are 3 bytes to represent the offset, the size of page is 111 = 0 to 7 = 8 bytes.
            > since there are 5 bytes to represent the page number, there can be maximum 11111 = 0 - 31 = 32 pages in this process.

        > note that the logical address is just imaginary since when the pages are put inside the ram, they are no longer in sequential manner
          and are put randomly, so how can page number work when the pages are distributed randomly.
        > Logical address is the only type of addressing a processor/CPU uses.
        > a cpu when in a process will simply ask for a byte using the Logical address format.(continued......)

    > Physical Address :
        > Physical address is how actually data is put inside of frames using pages in the Ram.
        > as the name suggests, this address actually exist physically.
        > Logical address comprises of Frame Number(in Binary) followed by Frame Offset(in Binary).
            > Frame Number : Ram is divided into various frames with each frame having indexing, this is frame number.
            > Frame Offset : the bytes inside of frame are also seperately indexed and represent the position of a byte of data present in frame.
        > ultimately, Logical address can help find a byte from any of the process present in the RAM using the frame concept and physical address.
        > Suppse Physical address is 010010101111(7 bytes for frame number and 5 bytes for frame offset)
          we can say the following about this logical address :
            > 0100101 = 37 and 01111 =15 hence the 37th frame and 15th byte is represented by this address.
            > since offset has 5 bytes hence size of each frame is : 11111 = 0-31 = 32 bytes.
            > since frame number is of 7 bytes the max number of frames in ram = 1111111 = 0 - 127 = 128 frames in RAM.

        > NOTE : when we stack up frame number and frame offset to represent a byte, that representation is same as the normal byte addressing.
                  eg. if ram has 32 bytes size and frames has size 4 bytes, then to represent a byte in 4th frame and 3rd position :
                    > Physical Address = 00100_011 = 35
                  and if we without condisering the frame counted the position of the same byte, it will be at 35th address only.
                  so the physical address may seem different from plain top to bottom addressing, it ultimately give the same address.

                  the same if true for logical address also
                  the address generated in logical address is same as the address that would be if the byte position was counted from index 0
                  in the process.

                  > note how we used 5 bits to represent frame number because of ram size and 3 bits for offset because of number of bytes in each frame.

        > The interaction with ram is always done in Logical address by the OS

    > MMU(MEMORY MANAGEMENT UNIT) and Page Table.
        > if OS uses physical address and cpu uses logial address, how can cpu interact with ram and make operation on address using OS??
        > MMU maintains a Page table for each individual process.
        > the reason why logical table is a failure is because it is with assumption that the process is in continuos pages form and also seperate
          paging is done for each process.
        > the Page Table consists the Frame number.
        > and no need to store page number since the frame number are in sequence of page number, normal pointer arithmetic can help access any
          frame number corresponding to the page frame.

>> NOTE : page table does not keep the page number as an entry since the frame number of corresponding page are stored in sequential order hence
          to access the nth page number just add n to the address of the first page table entry address.

            > the page table will tell for each process that in which frame a page is kept.
            > suppse CPU wants page 5 from a process, the Page table will help since it knows the frame number in which page 5 of that process is kept.
        > offset does not matter since in converting from logical to physical address and vice versa, the offset dosent change since page size = frame size.
        > size of a page table = number of frames*(size of page number address + size of frame number address)

>> NOTE : number of page entries = number of pages(and not number of frames,)
          also while size of frame and pages are equal, number of frames is not equal to number of frames.

    > HOW CPU AND OS INTERACT :

        > CPU is working on process 1 and want an address from process into address.
        > CPU prepares a logical address telling the particular location of the that byte wrt process page.
        > OS eith help of MMU opens the process 1 Page Table and finds the particular location of page requested by CPU.
        > it finds the page and goes to the frame which has it and goes to the offset location in the frame
        > it fetched the byte to the cpu.

        > Terminology :
            > LAS : Logical Address Space : size of the actual process in bytes.(space for LA)
                > if logical address space is 2^32, remember that the unit is bytes(since byte addressing).that then can be converted to gb
                  for finding the size of program.(the above its 4gb)
                  > the 2^32 is also the number of address in the program (since each byte has own address)
                  > how many bits are needed in a logical address space address representation ??
                    > in the above eg, 32 bits are needed to keep track of the highest address value in a 2^32 bytes address space.


                > later this space is divided into pages and logical address are genenrated as studied above.
                > number of pages = Logical Address space in bytes/ size of one page in bytes.

                > given size of a page eg : 4kb what is number of bits needed to represent page offset ?
                    > 4 kb = 2^12 bytes. hence page has 2^12 bytes and to represent 2^12 bytes , we need 12 bytes.

                > for the above what are the number of bits needed to represent the page number :
                    > number of pages = 2^32/2^12 = 2^20 and bits needed to represent 2^20 bits is 20;

>> NOTE : convert everything to 2^x and x will be the number of bits needed to address 2^x elements.

            > Virtual Address Space : number of bits in LAS address representation. if LAS = 2^ 32, VAS = 32 bits.

            > PAS : Physical Address Space : size of ram.
                      64 mb PAS = 2^26 bytes. we need 26 bits to reprenset address of 2^26 bytes.

                      > rest of the calculations are as above.

            > Numer of entries in Page Table : = number of pages (whichever is shorter)
                > note that its not necessary that the size of PAS > size of LAS.

            > size of a page table = number of frames * size of frame number address(there can be additional non mandatory entry also but lets ignore.)
                > since page table only keeps track of page number and frame numbe since offsets remain the same.


      >> Page Table Entry :
          > Frame Number --> Valid(1)/ Invalid(0) --> Protection(RWX) --> Reference(0/1) --> Caching(enable/disable) --> Dirty :

          > the above is the value stored in page table for each page number.
          > only Frame number is mandatory and rest are optional fields.
          > Valid/Invalid // Present/Absent: this is if page is present in the frame or not(we will study later virtual memory and find why a page
            might be absent)
          > Protection : the permission allowed on the page eg. read write execute (can be represented by 3 bits)
          > Reference : we can swap inn swap out in virtual memory (later study) was the page ever brought to memory before?? this is stored here.
                        if page never came here before ever, its 0.
                        ps. will make more sence when we study VM.
          > Caching : if data used multiple times, it is kept in cache. this is the switch that enables caching (HOLY SHITT !!)

>> NOTE : Page table contains entry for all the pages of a process, but not all the pages are brought into the ram at a time, hence if a page entry is there
          in the page table but the page is not in ram, the valid bit will be 1 i.e. invalid.

>> NOTE : we dont always put the most used data in the cache. The data must be most used and also non dynamic.
          non dynamic means, data that needs constant refreshing. eg. a user typing some input should not be cached since if user types new data
          the cache data will be used since cache values are absolute badass. Cache has constants and not changing data that is to be used frequently.

          > Dirty/Modify : if a page value has been modified in the ram, this is made into 1 so that the changes can be implimented in the secondary memory
              later after the program is done executing.
              this is better alternative to scanning the page with its old version(which is in hdd) to check if the data differes.

NOTE : page table base register (PTBR)  is the register that keeps track of the page table entries that are to be accesed. and Page tables are themselves stored in
        the main memory so this register comes in handy when traversing in a page table.

IMPORTANT > MULTI level Paging :
        > Multi Level paging is, when a page table is itself broken into pages(haha diserved that.) and a page table is made for the page table.
        > why do we do this though ?? lets find using an example :
            > LAS : 4gb = 2^32 bytes , PAS : 256 MB = 2^28 bytes , Frame size : 4KB = 2^12 bytes, Page Table Entry size = 2B.
              the number of entries in the page table = number of pages = 2^20
              since size of one page is 2b hence the total size of page table = 2^21 = 2MB

              notice anything??
              the page table is to be stored in the main memory and main memory itself has frames. if each frame is of size 4kb how do we fit a 2mb page table??
              we page the page table.

        > HENCE we do multilevel paging when a page table size is bigger than the frame size.

        > now the page table will be broken into smaller pages with size 4kb and an outer page table will be made for the page table.
        > calculating the size and values of the outer page table is simple maths and not different from a normal page table.

        > NOW if a page is to be found, the outer page table tells the location of innner page table the inner table tells the location of the page in the frames.

    >> NUMERICAL SOLVED :
        > LAS : 4gb = 2^32 bytes (size of process)
          PAS : 256 MB = 2^28 bytes  (Ram size)
          Frame size : 4KB = 2^12 bytes = page size
          Page Table Entry size = 2B. size of one entry of page

          page address bits = 32 bits
          number of pages = 2^32/2^12 = 2^20.
          logical address dimentions = 20bit(page number) 12bit(page offset)
          number of page entries = 2^20.
          total size of page table = 2^20* 2 = 2^21 = 2 mb

          hence we make an external page table :

          external page table one frame size = 4 kb(same aas above)
          the size of internal page table / logical address space : 2 mb = 2^21;
          number of entries in the table = LAS/size of one page = 2^9;
          in this also page table entry size : logical address bits = 21 where : 9 bits page number / 12 bits offset => entry size = 2^12 = 2 bytes.
          total size of the external page table = 2^9 * 2 bytes = 1kb;

    > Inverted Paging :
        > in normal paging we have to maintain page table for each process.
        > and also we have to find the pagetable in main mamory.
        > Problem ??
            > for 10 processess, we have to maintain 10 frames for their page tables. which is SHITT.

        > In inverted paging, there is a single page table made which is global.
        > such page table is called Global Page Table.
        > in normal paging, the outer indexing or addressing denoted page number and inside attribute was frame number
        > in inverted page number, outer indexing is frame number and inner entries are page number and process ID.

>> NOTE : in page table we kept track of pages kept in which frame, whereas in global page table, we keep track of frames have what page form what process.

        > now the page table is sequenced based on the sequence of frames rather than page.
        > also the number of page entries depends on size of PAS.

        > disadvantage : searching time is linear and hence time consuming.
        > inverted page table is memory efficient but falls back in time and hence was a failure.
            > since memory is getting cheaper and time is less.

>> NOTE : we can evaluate the size of sigle entry of a page using the page offset size?? wrong !! page offset size gives just the size of the page number attribute
          and as we know there can be additional entries in a page table like enable/disable, caching , dirty etc. hence if size of single entry given does not match
          the offset , DONT BE CONFUSED and if the size of offset is not given, it means we evaluate page table entry size using offset since the other fields are not
          mandatory.

>> the view using which cpu uses the logical address is the same manner in which the process was originally in the secondary memory, i.e in a continuos manner.

31.) Thrashing in OS :
    > The more the number of process we can call in the cpu, the more the cpu utilisation. This is known fact.
    > since the size of ram is limited, the proces is divided into pages in the secondary memory only and only a few pages are brought into ram at a time from a single process.
    > this way many process can be in the ram while only a few pages are coming into the ram actually.

    > using the above process, we achieved the best Degree of Multiprogramming but there is an issue :
        > since the whole process is not in the ram, what if processor needs a certain part of unloaded page ??
        > there is an page fault and page fault service time is used.(will see in Virtual Memory later)
            > in page fault service time, the desired page is to be brought into the ram, but it takes lot of time.

    > the whole os will become buisy in servicing the page faults only.
    > we know that degree of multiporgramming is directly proportional to cpu utilisation.
    > but this is true only upto a certain extent.
    >> we make the following observation on seeing the cpu utilisation and degree of multiprogramming graph :
        > Cpu utilisation increases with the increase in degree of multiprogramming.
        > on a certain level of degree of multiprogramming, the cpu utilisation reaches the highest peak.
        > after this, there are so many process in the ram and so little pages from a single process, that there is lots of page miss
            causing lots of page fault. this leads to cpu utilisation going down drastically.

NOTE : a table contains all the page entries for a process, suppose a page is not present in ram, its valid bit is set to 1 in the table.
            if cpu asks for a page, and the page valid bit is 1, that is calles a page miss.if 0 its a page hit.

        > the curve falls drasically upon increasing the degree of multiprogramming any further.

    >> THRASHING is when the increase in degree of multiprogramming starts causing deplition in CPU utilisation after a certain point.
        or
       Thrashing is when putting too many process in the ram causes too many page miss leading to cpu spending lots of time in fixing page
       fault that leads to decrease in cpu utilsation with increase in number of programs.

    > Solutions :
        > incrasing the size of main memory.(which is not cheap)
        > Long term scheduler is responsible for getting as many process in the ram, hence Long term scheduler can be asked to get the process
          only till the value at which the cpu utilisation reaches peak and no furter.

32.) Segmentation :
    > Issues with paging :
        > Paging is done without any consideration of the code.
        > eg. suppose there is a function add(), paging can split this function into two pages which is not ideal since this whole function has to
            work as a whole unit, and what if add starting page is in the main memory and the 2nd part is in another page in another address ?
            traversing will be too complex and not worth it.

    > Segmentation also splits process into segment but based on the code(controlled by user).
    > eg. function add() which needs to be whole will be in a single segment.
    > so major difference is that segments are unequal in size and are made based on the code whereas, pages are just equally sized chunks of process.

    > Just like paging, cpu works on logical address and logical address is to be converted into physical address.
    >> DEALING WITH SEGMENTED DATA :
        > a process gets divided into unequal sized segments and is placed into maim memory at random locations.
        > each segment has its unique segment base address and segement size.
            > segment base address is the actual starting byte address of the segment in main memory.
            > segment base address is the number of bits in the segment.
            > the physical address is made of base address and segment offset combined.

        > the CPU still deals with logical address and in this case logical address comprises of :
            > Segment Number and Segment size/Offset
            > the cpu is still unaware that the process is broken into pieces so it address the process thinking its a continuos process.
            > hence the logical address is the position of the segment, and the position of the byte to be addressed.

        > the MMU here too maintains a Segment Table which comprises of :
            > Base address and size.
            > the addressing of the table is done based on teh segment number sequence
                > this means 1st entry is about the first segment, second entry about second segment etc.
            > when cpu asks for a logical address, the MMU goes to the entry at particular segment number in the table.
              and then checks the Base address and size value to access the data in ram.

NOTE : this addressing is magical like page addressing lets understand : suppose physical address in segmentation was coming out to be 110100
        this physical address will point to a location in the ram right. When converted : 110100 = 52 and if we count from the top address of ram,
        the physical address 110100, which was made using segmentation, lies at 52nd position !!!!! the segmentaion and paging generated addressess
        be it physical of logical are same as the normal byte addressing addresses, the only difference is how the address 110100 is split into 2 parts,
        the base address, and the offset!!!

        > Dealing with error and TRAP :
            > suppose a logicla address has an offset that is bigger than the size of the Segment. what happens then ??
            > this is an error since the address the logical address is refering to does not belong to the segment the OS is asking off.
            > Hence trap is trigered and the operation is haulted.

        > There is a disadvantage that, there will be internal fragmentation

>>> Segmented Paging vs Paging segmentation :

    > paging and segmentation both have flaws and hence they are combinedly used :
    > the main flaw with pagingin is that most of the address in the page table are not valid, ie. most of the pages are not loaded in ram.
    > the following are the fusion of segmentation and paging :

        > Segmented Paging :
            > the process is divided into 4 segments traditionally called Code , Data, Stack and Heap.
            > after that a page table is made for each segment.
            > track of both segment table and page table.
            > the logical address can be split into segment number and offset
              upon going to the segment number using the segment table, we get the segment base and limit.
              (limit tells the size of physical memory that the page table is paging in main memory)
              the segment base value will act like a logical address and will point to page number and offset will tell the offset of the frame.
              using the page number we can find the frame number and upon going to the frame number and going to the offset, we get the desired byte.

            > Advantages :
                > the page table is only made for the desired segments so size of page table is reduced.
                > much better view
                > reduced ecternal fragmentation.

            > Disadvantages :
                > Internal fragmentation still exists
                > Hardware support needed.

        > Paged Segmentation :
            > used when the size of segments becomes way larger than size of frame so the segmentation table is also split into pages
            > paged segmentation is segmented paging with segment table itself split into pages.
            > in segmented paging, not every process has same number of segments and varying size,leading to varying segment sizes
              leading to external fragmentation.
            > so how to improve upon stuff ??
            > logical address points to a paging table that leads to segmented table that is again leadint to a page table for each segment.

            > Advantages :
                > No external fragmentation
                > Reduced memory requirements as no. of pages limited to segment size.

            > Disadvantages :
                > Internal fragmentation remains a problem.
                > Hardware is complexer than segmented paging

                  > > THIS CAN BE STUDEIED FURTHER ON JAVA POINT
                  OR

>> SEGMENTED PAGING :
Program is broken into segments and each segment address space is proken into pages
    > Hence segment table will take to the page table and page table will take to the frame where data is present.

>> PAGED SEGMENTATION :
When the size of segmented table is very larger than a frame/page size, the segment table is broken into pages
    > hence page table for segmented table will take to the segment, segment will guide to the page table and page table to frame where data
      is present.

33.) Overlay :
    > this was used in premitive devices where the memory was partitioned.
    > If a process has bigger size than the main memory, still we can put the process into main memory with the help of overlay.
    > overlay can also be used suppose there is a static partitioning and a process has size bigger then the biggest partition.
    >> this is used in Embedded Systems because memory is limited in embedded systems and hence overlay can help execute big process
        in a limited sized memory.

    > the concept says that a process big in size can be made in such a way by the user that it can be split into chunks and placed in memory
      in chunks one after another.
    > the reason this is to be done by the coder is to help driver , there is a driver for doing fancy stuff in embedded softwares.
    > also the coder has to make sure that the partitions that are made, are not interdependant, but fully indipendant.

    > this concept is only used in embedded system since the functionality is fiexed , whereas in computers, the programs are of varying sizes
      and perform different functionality and also interdependant. hence only embedded.



    > eg. question  :
        > Consider a Two Pass Assembler : pass1 : 80kb, pass2 : 90kb
          Symbol Table : 30kb
          Common Routine : 20kb
          At a time only one pass is in use , what is min partition size required if overlay driver size is 10kb.

        > in the above system, we know that since pass1 and 2 are not dependant hence we shall run them one at a time.
          other than pass1 and pass2, all the other are mandatory resources for a pass to run.
          both passes need all the other resoures to work hence :
          for Pass 1 : 80kb + 30kb + 20 kb + 10 kb = 140kb.
          for pass 2 : 90 kb + 30kb + 20 kb + 10 kb = 150kb.

          hence the minimum size of the partion for the above embedded system has to be 150 kb and the assembler can run usng the concept of
          overlay, with one pass at a time.

    >> in normal system also we need to run one part of a program at a time but it is done using the concept of virtual memory.



34.) Virtual Memory :
    > it gives illusion that a process that has size bigger than main memory size can be executed.
    > here too the process is divided into various pages and instead of bringing all the pages to main memory, only some are brought into
      the ram and swapping of pages is done giving this illsion.
    > Advantage is that we can bring in multiple process pages in the ram.

    > Page replacement has following operations :
    > Bringing pages inn ram from LAS is called swap inn or roll inn
    > Bringing pages out of ram back to LAS is called swap out or roll out.

>> Pages brought together in the main memory are related to one another i.e. pages that are adjecent in the LAS.

    > This is used everywhere today.

    > Disadvantage :
        > like we studied in thrashing, if too many process are brought inn, the number of pages from each process is reduced and hence chance
          of page miss/ page fault increase.

>> Dealing with Page Fault :
    > when a cpu requests a page, and if the page is invalid(i.e. not in ram) a page fault occures, an interrupt(trap) is generated and cotrol
      goes form user to kernal mode. OS now checks the validation of the program(Checks permissions and user etc to allow swapping inn of pages)
      the missing page is swapped in from logical memory into a free frame, the frame is updated in the page table. now the invalid is set to valid in the
      page table and the control goes from kernal back to user. Now the cpu can access the missing page.

    > these steps only occur when the page is invalid and trap is generated hence there is page fault.

    > the 6 steps taken to deal with the page miss, take a lot of time.

  >> Page Fault Service time(milli seconds) :
       > the time needed to fetch a page from secondary memory to main memory and update the page table after the trap is generated.
       > or we can say it is the time under which a page fault is dealt with.
       > this is very slow (1 milli second = 1000000 nano seconds)
       > this occures when there is page miss.

  >> Main Memory Access time(nano seconds) :
      > the time needed for os to swap inn a page from the main memory and give it to the os who requested the page.
      > it is very fast since main memory is very fast.
      > this occures when there is a page hit.

  >> Effective Memory Access Time(EMAT) : this is the average time taken by a system(with processor,memory and os) to fetch a memory for processor
                                        from memory.
    if p = probability of page fault occuring .
    EMAT = p(page fault service time + main memory access time) + (1-p)(main memory access time).

    this shows how efficient an os is at avoiding page fault(thrashing).
    > note that when page fault occures, the total time taken to fetch is page fault service time + main memory access time
        since PFST is time to fetch and update a page table entry whereas the MMAT is time taken to access the newly swapped in entry.


35.) TLB (Translation Lookaside Buffer) in CACHE
    > we know how pages and frame work, how logical address is converted into physical address.
    > Now we know that the Main Memory Access Time is the time needed to fetch a request made by cpu from main memory.

    > the conversion of logical address to physical address is done through page table and we know that page table itself is stored in the main memory.
      hence if x amount of time is needed to access data in ram, x time will be needed to fetch a page table entry from ram.
      HENCE time needed to convert logical address to physical address = time needed to fetch data from ram.
      so what will be time needed to convert logical address to physical address and fetch the physical address data ??
      it will be x+x = 2x.

>>    > Cache memory is of two types : I- Cache and D- Cache
        > I cache is Instruction cache and stores instruction that can be executed(eg. functions, logic etc)
        > D cache is Data cache and stores data values(eg. variables, address values etc.)

      > In Virtual memory oriented OS and processors, D cache has a protion called Translation Lookaside Buffer.
      >> TLB :
          > TLB stores some portions of page table for faster conversion of Logical address to Physical address.
          > TLB space is limited and hence most used entries from Page table are sored in the TLB
          > TLB indexes are called tags.
          > when CPU wants a logical address, it will check if the page table entry exists in the TLB
              > if its present in tlb its called TLB hit.
              > if its not present in TLB its called TLB miss.

    > if the access time of data from cache is tlb(suppose) and time taken to fetch data from ram is x then :
        > total time to fetch data from main memory when its s TLB hit = tlb+x
        > total time to fetch data from main memory when its a TLB miss = tlb+x+x(since first tlb will be checked for entry)
        > ratio of tlb hit to miss is : hit : miss

    >> Hence the EMAT(Effective Memory Access Time ) when there is no page fault = hit*tlb hit time + miss*tlb miss time = hit(tlb+x) + miss(tlb + 2x)

NOTE : we have calculated 2 EMAT values : EMAT when there is page fault and EMAT when there is no Page fault.

36.) Page Replacement and Page Relplacement algorithms :
    > Swapping in of memory can be done easyily if there is space left in the main memory. But what if main memory is full??
    > in such case a page in main memory has to be replaced by a page that cpu has requested.
    > this is common occurence since generally the size of process and number of process to be run at a time is far greater than ram.

    > there are various algorithms to deal with page replacement :
        > FIFO
        > Optimal Page Replacement
        > Least Recently Used(LRU)
    > these algorithms help reduce the page faults dractically.
>> NOTE : page replacement will only come in play when ram is full. so it becomes important to keep in mind which pages are most important and neede in ram, since
          removing a commonly requested page can result in more page miss and increasing EMAT.

    > FIFO :
        > the cpu requests page in a queue format or we can say it requests a series of address one after another.(each individual address requested is reference)
        > in FIFO when the main memory if full and a page is to be swapped inn, the frame that came inn first is removed.
        or
        > the page that arrived first in main memory is removed and gets replaced by fresh requested page.
        > hit ratio of fifo = number of page hits using fifo/number of reference requested by os

        > Belady's Anomaly :
            > this is an anomaly that occures in fifo page replacement.
            > we know that increasing the number of frames(size of ram) should decrease the number of page faults but in cases in fifo increasing
                the number of free frames available will result in more page faults.
            > such example is attatced in Belady'sAnomalyInFIFO.png
            > note that cpu geenerally requests series of continuos references in logical address from the os, and in such example where the sequence of page
              requests made by cpu are continuos result in such anomaly .(eg. 1,2,3,4,1,2,5,1,2,3,4,5)


    > Optimal Page Replacement ALgo(replace the page that is needed last in the coming future) :
        > in this upon there being need for page replacement, the sequence of upcoming reference requests are checked and the page that is not used in the longest
          dimension of time in future is replaced.
        or
        > the cpu generates a series of referenced(logical) to be fetched, in that seires the page that which is in the ram but will not be used for the longest period
          of time will be replaced or the one with no demand will be replaced.
          eg. in 123432341, if page is to be replaced from 1,2,3,4, ==> 4 will be replaced since 1,2,3 are being called before 4 in the future.

NOTE : we are able to determine which page will be needed after the longest time since cpu generates a list(called Reference String) before hand.

        > REMEMBER WE ARE LOOKING AT THE FUTURE.
        > note we replace the page that has no demand first.

    > Least Recently used(Replace the least recently used in the past)
        > in this page replacement algorithm when it comes to replace a page, we replace the page which was called least recently in the past
          or
          we replace the page that was used most time ago in the past.
        > note that we will replace the page that was not requested in the past first.
        > although this algorithm gives less page faults, it is slow since there is searching algorithm that has to work to detect the least recently accessed page.
        
    > Most Recently used(Replace the most recently used page in the past)

37.) Disk Architecture :
    > inside a harddisk :
        > there are various Platters. Platters are discs that have a center hole.
        > these discs have a Spindal. Spindal is a rod that passess through the center of all the Platters.
            > spindal is responsible for rotating the discs in a direction and are unidirectional.
            > the paltters are in a stack from and symmetically above each other with some space between them.

        > the discs have surfaces on both the sides. There surfaces are where the data is stored .
            > two surfaces are upper and lower surface.
        > above the platters there are read/write heads.(like the pin in a gramophone)
            > readwrite heads are on both the surfaces of the disc and can be seen in the attatched image.
            > the readwrite heads move back and forth to read or write from the disc.
        > all the read/write head are connected to a common Actuator arm.
        > the Platter surfaces have round and round tracks on both the surfaces.
            > when a read/write arm has to change track, it does so using the back and forth movement.
NOTE : read write heads just move back and forth and the roatation is done by the discs.

        > now even every track is divided into sectors, sectors.
            > the number of sectors are fixed(note the inner tracks have smaller diameter)
            > the data is stored in the sectors.
        >> Platters ==> Surface ==> Track ==> Sectors ==> Data
        >> number of surfaces = number of platter*2
        >> number of tracks = (number of platters)*2*(number of tracks on one surface)
        >> number of sectors = (number of platters)*2*(number fo sectors in one track)*(number of tracks on one surface)

        >> Total Disc Size = 2*(Number of platters)*(Number of tracks per surface)*(Number of sectors per track)*(Size of one sector)

38.) Disc Access Time :
    > Seek Time : Time takes by R/W head to reach desired track.
        > it is the time taken for the read write head to reach the desired track in which the desired data resides.
        > NOTE : just goint to the track and not going to the data , keep that in mind.
        > In best case, the data to be fetched is on the same track where the head is .
        > In the worst case, the read Write head can be on the outermost/innermost and the desired track is innermost/outermost respectively.
        > The seek time is most critical and can determine the speed of accessing data.
        > But we generally take average and it will be = worst case time / 2.

    > Rotation Time : Time taken for one full Rotation(360 degree).
        > NOW the disc rotates(spindal rotates it) so that the read write head can reach desired sector.
        > The time taken for the read/write head to do a complete 360 degree rotation is called Rotation time.

    > Rotational Latency : Time takes to reach to desired sector
        > after seek time is spent, the read write head has reached the desired track and now correct sector is to be accessed.
        > the time taken to go from the current sector to current sector is called Rotational Latency.
        > in the best case the data is on the same sector as the current .
        > in the worst case the data can be on the farthest from current sector.
            > notice that since the spindal is unidirectional rotation, the farthest sector is actually the adjecent of current sector
              but in the opposite direction.(360 degrees that is = rotation time)
        > but we take the average hence we take rotation time/2
            > this can also be said, time takes for read write head to go from current sector to 180 degrees.

    > Trancefer Time : it is the time needed to transfer a given amount of data .
        > Transfer Time = amount of data to be transfered/ speed of transfering(seconds) or data size/transfer rate

    > Transfer Rate/ Data rate : amount of data that can be transfered in one unit time(second).
        > Transfer rate = size of data/transfer time.
        > Transfer rate = {Number of heads * capacity of one track * no of rotation in one seconds}
            > number of heads is equal to number of surfaces.
            > capacity of one track is the amount of data that can be stored in one track.
            > number of rotations is number of times the disck is rotated by the spindal in one second(measured in : RPM)

    > there is sometimes Controller Time(CT) given which is to be added at the end.
        > controller superwises disc operations.
    > also there is Queue Time(QT) which again is to be added at the end.
        > when too many requests are sent to the hdd, the requests are kept in queue.
        > the time spent in the queue is also added in the end to the computed transfer time.

    >> DISC ACCESS TIME = Seek Time + Rotation Time + Transfer Time + Controller Time(if given) + Queue Time(if given)
